# Books Scraper API

## Descri√ß√£o do Projeto

A **Books Scraper API** √© uma aplica√ß√£o desenvolvida em Python com FastAPI que realiza web scraping do site [Books to Scrape](https://books.toscrape.com/) e disponibiliza os dados coletados atrav√©s de uma API REST documentada com Swagger.

### Objetivo
Coletar informa√ß√µes de livros (t√≠tulo, pre√ßo, categoria, disponibilidade, avalia√ß√£o e descri√ß√£o) e disponibilizar esses dados atrav√©s de endpoints REST para consulta, busca e an√°lise.

### Funcionalidades Principais
- **Scraping Automatizado**: Coleta dados de todas as p√°ginas do site
- **Busca Avan√ßada**: Filtros por t√≠tulo e categoria
- **Monitoramento**: Status em tempo real do processo de scraping
- **Documenta√ß√£o Interativa**: Swagger UI integrado
- **Organiza√ß√£o por Tags**: Endpoints organizados por funcionalidade
- **Arquitetura Modular**: C√≥digo organizado em m√≥dulos especializados
---

## Arquitetura do Projeto

```
tech_challenge_1/                # Reposit√≥rio Git
‚îú‚îÄ‚îÄ main.py                      # Aplica√ß√£o principal FastAPI
‚îú‚îÄ‚îÄ requirements.txt             # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ routes.py                # Defini√ß√£o das rotas da API
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ web_scraper.py           # Orquestrador principal de scraping
‚îÇ   ‚îú‚îÄ‚îÄ html_parser.py           # Parser HTML com BeautifulSoup
‚îÇ   ‚îî‚îÄ‚îÄ scraper_utils.py         # Utilit√°rios e helpers de scraping
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ scraping_books_database.csv  # Base de dados CSV (gerado automaticamente)
‚îî‚îÄ‚îÄ README.md                    # Documenta√ß√£o do projeto
```

### Componentes da Arquitetura

1. **FastAPI Application** (`main.py`)
   - Configura√ß√£o da aplica√ß√£o principal

2. **API Routes** (`api/routes.py`)
   - Endpoints REST organizados por tags
   - Modelos Pydantic para valida√ß√£o
   - Documenta√ß√£o detalhada com exemplos

3. **Web Scraper Service** (`services/web_scraper.py`)
   - Classe `WebScraper` para orquestra√ß√£o do scraping utilizando BeautifulSoup
   - Gerenciamento de estado com `ScrapingState`


4. **HTML Parser** (`services/html_parser.py`)
   - Classe `HTMLParser` para parsing HTML
   - Extra√ß√£o especializada de dados dos livros
   - Processamento de elementos espec√≠ficos do site

5. **Scraper Utils** (`services/scraper_utils.py`)
   - Classe `ScraperUtils` com fun√ß√µes utilit√°rias
   - Opera√ß√µes de rede e manipula√ß√£o de dados
   - Fun√ß√µes auxiliares para valida√ß√£o e formata√ß√£o

6. **Data Storage** (`data/`)
   - Armazenamento em CSV
   - Estrutura de dados com pandas DataFrame
---

## Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- Python 3.8 ou superior
- Git
- pip (gerenciador de pacotes Python)
- Conex√£o com a internet

### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/gablula/tech_challenge_1.git
```

### 2. Crie um ambiente virtual
```bash
# Windows PowerShell
python -m venv venv

# Linux/macOS
python3 -m venv venv
```

### 3. Ative o ambiente virtual
```bash
# Windows PowerShell
.\venv\Scripts\Activate.ps1

# Windows CMD
venv\Scripts\activate.bat

# Linux/macOS
source venv/bin/activate
```

### 4. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

**Depend√™ncias inclu√≠das:**
- fastapi==0.115.0
- uvicorn[standard]==0.31.1
- pandas==2.2.3
- requests==2.32.3
- beautifulsoup4==4.12.3

---

## Instru√ß√µes para Execu√ß√£o

### Executando a API

#### M√©todo 1: Execu√ß√£o Direta
```bash
# Com ambiente virtual ativado, execute a partir da raiz do projeto
python main.py
```

**Nota:** Voc√™ pode configurar `HOST` e `PORT` via vari√°veis de ambiente (ex: `export HOST=127.0.0.1 PORT=8080`).

#### M√©todo 2: Usando Uvicorn
```bash
# Com ambiente virtual ativado
uvicorn main:app --host 0.0.0.0 --port 8001 --reload
```

### Acessando a Aplica√ß√£o

Ap√≥s iniciar o servidor, a API estar√° dispon√≠vel em:
- **API Base**: http://localhost:8001
- **Documenta√ß√£o Swagger**: http://localhost:8001/docs
- **Documenta√ß√£o ReDoc**: http://localhost:8001/redoc
- **Schema OpenAPI**: http://localhost:8001/openapi.json

### Workflow de Uso

1. **Inicie a API** conforme instru√ß√µes acima
2. **Acesse a documenta√ß√£o** em http://localhost:8001/docs
3. **Inicie o scraping** chamando `/api/v1/scraper/start`
4. **Monitore o progresso** em `/api/v1/scraper/status`
5. **Consulte os dados** quando o status for 'Done'

---

## Documenta√ß√£o das Rotas da API

### Tags e Organiza√ß√£o

As rotas est√£o organizadas em tr√™s categorias principais:

#### **Scraping** - Controle do Processo
- Gerenciamento do processo de coleta de dados
- Monitoramento de status e progresso

#### **Livros** - Consulta de Dados
- Busca e listagem de livros coletados
- Filtros por t√≠tulo e categoria

#### **Sistema** - Monitoramento
- Health checks e status da API

### Lista Completa de Endpoints

| M√©todo | Endpoint | Tag | Descri√ß√£o |
|--------|----------|-----|-----------|
| `GET` | `/api/v1/scraper/status` | Scraping | Status do processo de scraping |
| `GET` | `/api/v1/scraper/start` | Scraping | Iniciar scraping em background |
| `GET` | `/api/v1/scraper/reset` | Scraping | Resetar processo e limpar dados |
| `GET` | `/api/v1/books` | Livros | Listar t√≠tulos de todos os livros |
| `GET` | `/api/v1/books/search` | Livros | Buscar livros por filtros |
| `GET` | `/api/v1/books/categories` | Livros | Listar todas as categorias |
| `GET` | `/api/v1/books/{id}` | Livros | Obter livro espec√≠fico por ID |
| `GET` | `/api/v1/health` | Sistema | Health check da API |

---

## Exemplos de Chamadas e Responses

### **Scraping Endpoints**

#### 1. Verificar Status do Scraping
```http
GET /api/v1/scraper/status
```

**Response:**
```json
{
  "status": "Done",
  "books_count": 1000,
  "message": "Scraping conclu√≠do com sucesso. 1000 livros coletados."
}
```

#### 2. Iniciar Scraping
```http
GET /api/v1/scraper/start
```

**Response (Sucesso):**
```json
{
  "message": "Scraping iniciado em background."
}
```

**Response (J√° em execu√ß√£o):**
```json
{
  "message": "Scraping j√° est√° em execu√ß√£o. 450 livros coletados at√© agora."
}
```

#### 3. Resetar Scraping
```http
GET /api/v1/scraper/reset
```

**Response:**
```json
{
  "message": "Base de dados de scraping apagada com sucesso."
}
```

### **Livros Endpoints**

#### 4. Listar T√≠tulos dos Livros
```http
GET /api/v1/books
```

**Response:**
```json
[
  "A Light in the Attic",
  "Tipping the Velvet",
  "Soumission",
  "Sharp Objects",
  "Sapiens: A Brief History of Humankind"
]
```

#### 5. Buscar Livros com Filtros
```http
GET /api/v1/books/search?title=python&category=Programming
```

**Response:**
```json
[
  {
    "title": "Learning Python",
    "price": "¬£30.23",
    "category": "Programming",
    "availability": "In stock (19 available)",
    "rating": "Four",
    "description": "Get a comprehensive, in-depth introduction to the core Python language..."
  }
]
```

#### 6. Listar Categorias
```http
GET /api/v1/books/categories
```

**Response:**
```json
{
  "categories": [
    "Art",
    "Biography", 
    "Business",
    "Fiction",
    "History",
    "Poetry",
    "Programming",
    "Science"
  ]
}
```

#### 7. Obter Livro por ID
```http
GET /api/v1/books/0
```

**Response:**
```json
{
  "title": "A Light in the Attic",
  "price": "¬£51.77",
  "category": "Poetry",
  "availability": "In stock (22 available)",
  "rating": "Three",
  "description": "It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition."
}
```

**Response (N√£o encontrado):**
```json
{
  "detail": "Book item not found"
}
```

### **Sistema Endpoints**

#### 8. Health Check
```http
GET /api/v1/health
```

**Response:**
```json
{
  "api_status": "online",
  "uptime_seconds": 3661.45,
  "scraping_status": "Scraping conclu√≠do com sucesso. 1000 livros coletados.",
  "books_scraped": 1000,
  "csv_file_size_mb": 2.456,
  "csv_creation_date": "2025-11-01 14:30:15"
}
```

**Response (Scraping em andamento):**
```json
{
  "api_status": "online",
  "uptime_seconds": 1234.56,
  "scraping_status": "Scraping em execu√ß√£o. 450 livros coletados at√© agora.",
  "books_scraped": 450,
  "csv_file_size_mb": 1.234,
  "csv_creation_date": "2025-11-01 14:00:00"
}
```

**Response (Scraping n√£o iniciado):**
```json
{
  "api_status": "online",
  "uptime_seconds": 567.89,
  "scraping_status": "Scraping n√£o foi iniciado ainda.",
  "books_scraped": null,
  "csv_file_size_mb": null,
  "csv_creation_date": null
}
```

---

## Exemplos de Uso com cURL

### Iniciar Scraping
```bash
curl -X GET "http://localhost:8001/api/v1/scraper/start" \
     -H "accept: application/json"
```

### Buscar Livros de Fic√ß√£o
```bash
curl -X GET "http://localhost:8001/api/v1/books/search?category=Fiction" \
     -H "accept: application/json"
```

### Listar Categorias
```bash
curl -X GET "http://localhost:8001/api/v1/books/categories" \
     -H "accept: application/json"
```

### Obter Status
```bash
curl -X GET "http://localhost:8001/api/v1/scraper/status" \
     -H "accept: application/json"
```

### Health Check
```bash
curl -X GET "http://localhost:8001/api/v1/health" \
     -H "accept: application/json"
```

---

## üîó Exemplos de Uso com Python Requests

```python
import requests

# Base URL da API
BASE_URL = "http://localhost:8001"

# 1. Iniciar scraping
response = requests.get(f"{BASE_URL}/api/v1/scraper/start")
print(response.json())

# 2. Verificar status
response = requests.get(f"{BASE_URL}/api/v1/scraper/status")
print(response.json())

# 3. Buscar livros por categoria
response = requests.get(f"{BASE_URL}/api/v1/books/search", 
                       params={"category": "Fiction"})
print(response.json())

# 4. Obter livro espec√≠fico
response = requests.get(f"{BASE_URL}/api/v1/books/0")
print(response.json())

# 5. Listar categorias
response = requests.get(f"{BASE_URL}/api/v1/books/categories")
print(response.json())

# 6. Health check
response = requests.get(f"{BASE_URL}/api/v1/health")
print(response.json())
```

---

## Estrutura de Dados

### Campos dos Livros
Cada livro coletado cont√©m os seguintes campos:

| Campo | Tipo | Descri√ß√£o | Exemplo |
|-------|------|-----------|---------|
| `title` | string | T√≠tulo do livro | "A Light in the Attic" |
| `price` | string | Pre√ßo formatado | "¬£51.77" |
| `category` | string | Categoria do livro | "Poetry" |
| `availability` | string | Status de disponibilidade | "In stock (22 available)" |
| `rating` | string | Avalia√ß√£o em palavras | "Three" |
| `description` | string | Descri√ß√£o completa | "It's hard to imagine..." |

### Estados do Scraping
O processo de scraping pode estar nos seguintes estados:

| Estado | Descri√ß√£o |
|--------|-----------|
| `Idle` | Processo parado, pronto para iniciar |
| `Running` | Coletando dados em background |
| `Stopping` | Parando o processo atual |
| `Done` | Scraping conclu√≠do com sucesso |
| `Error` | Erro durante o processo |

---

## Deploy em Produ√ß√£o

### Deploy no Render

A aplica√ß√£o est√° configurada para deploy autom√°tico no Render com suporte a vari√°veis de ambiente:

1. **Conecte o reposit√≥rio** ao Render
2. **Configure as seguintes op√ß√µes:**
   - **Build Command**: `pip install -r requirements.txt`
   - **Start Command**: `uvicorn main:app --host 0.0.0.0 --port $PORT`
   - **Root Directory**: `./` (raiz do reposit√≥rio)

3. **Vari√°veis de ambiente** (opcional):
   - **HOST**: `0.0.0.0` (padr√£o, aceita conex√µes externas)
   - **PORT**: `8001` (padr√£o)

4. **Acesse a aplica√ß√£o** no URL fornecido pelo Render

### Exemplo de URL de Produ√ß√£o
- **API**: https://tech-challenge-1-r7gr.onrender.com
- **Documenta√ß√£o**: https://tech-challenge-1-r7gr.onrender.com/docs
- **Health Check**: https://tech-challenge-1-r7gr.onrender.com/api/v1/health
---


### Configura√ß√£o de Armazenamento
O arquivo CSV √© salvo em `./data/scraping_books_database.csv` automaticamente quando o scraping √© executado. Para alterar o local, edite a vari√°vel `SCRAPING_DATABASE_FILE` em `services/web_scraper.py`.

---

## Suporte

Para suporte e d√∫vidas:
- Email: gabriel.wensev@gmail.com
- Documenta√ß√£o: http://localhost:8001/docs
- Issues: Abra uma issue no reposit√≥rio

---